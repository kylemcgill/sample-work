'''
Parses a DataFrame file in chunks for resource constrained systems.
'''
import pandas as pd

def splitToCOlumnFiles(indir, outdir, infilename, file_type_name, microsoft_dtypes):
    # File is too big to be read in all at once, so grab the iterator
    df = pd.read_csv(indir + infilename, iterator=True, chunksize=50000, dtype=microsoft_dtypes) 


    # Look at the data to see how many NaN's there are
    # Need to set up counters
    number_of_nan_per_col = {}
    number_of_categories_per_col = {}
    count = 0
    new_nan_count = 0
    new_category_set_count = 0


    # Reads 'chunksize' rows in from the Iterator and puts them into a dict
    for chunk in df:
        # Progress bar
        print('Iteration: ', count)

        machine_id_col = chunk['MachineIdentifier']
        has_detections = chunk['HasDetections']

        for col in chunk:
            if True: # (col != 'MachineIdentfier') & (col != 'HasDetections'):
                current_df = pd.concat([chunk[col]] , axis=1)
                write_header = False
                if count == 0:
                    write_mode = 'w'
                    write_header = True
                else:
                    write_mode = 'a'
                    write_header = False
                current_df.to_csv(outdir  + col + file_type_name, mode=write_mode, index=False, header=write_header)

                # print(current_df)
        
        count += 1
